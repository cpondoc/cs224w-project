{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8rvGO8rI-ww"
   },
   "source": [
    "# Exploring the Impacts of Architecture and Scale on GNN Performance on Relational Data\n",
    "By: Joseph Guman, Atindra Jha, and Christopher Pondoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome back to Relbench! In this tutorial, we'll dive a bit deeper into the benchmark + Relational Deep Learning and explore several choices around architecture, scale, and generalizability. In particular, we'll look to answer the following questions:\n",
    "\n",
    "1. Can we train our Relational Deep Learning on one entity classification task and expect strong zero-shot performance on another entity classification task? What happens if we finetune the model?\n",
    "2. How does our choice of using embedding models to generate expressive node features impact our performance on node classification tasks?\n",
    "3. How can we alter and/or extend the architecture of our existing Relational Deep Learning model to improve performance on different tasks?\n",
    "\n",
    "This notebook already assumes you've looked through the tutorials on [loading in data](https://github.com/snap-stanford/relbench/blob/main/tutorials/load_data.ipynb) and [training a model](https://github.com/snap-stanford/relbench/blob/main/tutorials/train_model.ipynb), as our walkthrough uses those guides as a launchpad to explore deeper questions. If you haven't had a chance to look through those notebooks, we suggest starting there first.\n",
    "\n",
    "With all that being said, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0: Why Relational Deep Learning\n",
    "Before getting started, let's motivate the use of Relational Deep Learning by defining a non-GNN baseline to compare against. One of the main issues with older methods -- such as tabular methods or even just using standard statistical ML algorithms -- is that they either only work on one table or require lots of heavy feature engineering to reap the benefits of GNNs. On the other hand, Relational Deep Learning allows us to learn directly on relational data without feature engineering.\n",
    "\n",
    "For our baseline, let's use a classical statiscal model. In particular, let's use [LightGBM](https://lightgbm.readthedocs.io/en/stable/), which is a gradient boosting framework that uses tree-based learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LightGBM baseline\n",
    "from src.models.baseline import train_model\n",
    "\n",
    "train_metrics, val_metrics, test_metrics = train_model(\"rel-f1\", \"driver-dnf\")\n",
    "\n",
    "# Print corresponding statistics\n",
    "print(f\"Train: {train_metrics}\")\n",
    "print(f\"Val: {val_metrics}\")\n",
    "print(f\"Test: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It looks like just by taking together all of raw tables and merging them together, we're able to achieve reasonable baseline performance. Let's see if we can do better without having to merge together all of the tables, though, with Relational Deep Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Can we generalize?\n",
    "Let's take a look at our first question, which involves looking at whether our Relational Deep Learning model can generalize to other tasks with/without finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start by looking setting up Relbench. As with the other tutorials, we're taking a look at the `rel-f1` dataset and focusing on node classification tasks. We'll begin by training a model on the `driver-dnf` task, which predicts whether a driver will not finish a race in the next month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tasks.tasks import initialize_task, db_to_graph\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch_geometric.seed import seed_everything\n",
    "\n",
    "# Set up dataset and task, define metrics and loss\n",
    "dataset, task, train_table, val_table, test_table = initialize_task(\n",
    "    \"rel-f1\", \"driver-dnf\"\n",
    ")\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "# Set up device\n",
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then preprocess all of our Relbench data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from relbench.modeling.graph import make_pkey_fkey_graph\n",
    "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
    "from src.embeddings.glove import GloveTextEmbedding\n",
    "\n",
    "# Preprocess the database data and set up our text embedder\n",
    "db, col_to_stype_dict = db_to_graph(dataset)\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=GloveTextEmbedding(device=device), batch_size=128\n",
    ")\n",
    "\n",
    "# Load in data used to train model\n",
    "root_dir = \"./data\"\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=text_embedder_cfg,\n",
    "    cache_dir=os.path.join(root_dir, f\"rel-f1_materialized_cache\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load in the data and have our model set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.loader import get_loader\n",
    "from src.models.rdl.graph_sage import RDLModel\n",
    "\n",
    "# Set up data loader and model\n",
    "loader_dict, entity_table = get_loader(train_table, val_table, test_table, task, data)\n",
    "model = RDLModel(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=2,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(device)\n",
    "\n",
    "# if you try out different RelBench tasks you will need to change these\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize, let's initialize our training run, and evaluate our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.training import eval_model, training_run\n",
    "\n",
    "# Get model after a training run\n",
    "state_dict = training_run(\n",
    "    model, device, optimizer, task, loader_dict, val_table, loss_fn, entity_table\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate on val and test set\n",
    "eval_model(model, loader_dict, \"val\", task, device, val_table)\n",
    "eval_model(model, loader_dict, \"test\", task, device, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we are able to roughly replicate the results from the [core Relbench paper](https://huggingface.co/spaces/relbench/leaderboard). However, do the results generalize? To do so, let's load in the data for the other entity classification task within `rel-f1` -- `driver-top3` -- and see how we do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Results\n",
    "part1_top3_labels = [\"Trained on DNF, Zero-Shot on Top 3\"]\n",
    "part1_top3_scores = []\n",
    "\n",
    "# Reuse functions to set up `driver-top3 task`\n",
    "dataset, task, train_table, val_table, test_table = initialize_task(\n",
    "    \"rel-f1\", \"driver-top3\"\n",
    ")\n",
    "db, col_to_stype_dict = db_to_graph(dataset)\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=text_embedder_cfg,\n",
    "    cache_dir=os.path.join(root_dir, f\"rel-f1_materialized_cache\"),\n",
    ")\n",
    "\n",
    "loader_dict, entity_table = get_loader(train_table, val_table, test_table, task, data)\n",
    "model = RDLModel(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=2,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "test_metrics = eval_model(model, loader_dict, \"test\", task, device, None)\n",
    "part1_top3_scores.append(test_metrics[\"roc_auc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, trying out our model zero-shot does not yield amazing results. However, what happens if we use this model as a starting point for finetuning on the task? Let's experiment on fine-tuning this model with fewer epochs on the `driver-top3` task and checking its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Results\n",
    "part1_top3_labels.append(\"Trained on DNF, Finetuned on Top 3\")\n",
    "\n",
    "# Get model after a training run\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "state_dict = training_run(\n",
    "    model,\n",
    "    device,\n",
    "    optimizer,\n",
    "    task,\n",
    "    loader_dict,\n",
    "    val_table,\n",
    "    loss_fn,\n",
    "    entity_table,\n",
    "    epochs=5,\n",
    "    state_dict=state_dict,\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate on val and test set\n",
    "eval_model(model, loader_dict, \"val\", task, device, val_table)\n",
    "test_metrics = eval_model(model, loader_dict, \"test\", task, device, None)\n",
    "part1_top3_scores.append(test_metrics[\"roc_auc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It looks like after we finetune even after just one epoch. we're able to practically replicate the Relbench results. Finally, let's compare this approach to simply training on the task from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Results\n",
    "from src.models.viz import create_bar_graph\n",
    "part1_top3_labels.append(\"Trained on Top 3 Task\")\n",
    "\n",
    "# Define a new model, don't load in old weights.\n",
    "base_model = RDLModel(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=2,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(device)\n",
    "base_optimizer = torch.optim.Adam(base_model.parameters(), lr=0.005)\n",
    "base_state_dict = training_run(\n",
    "    base_model,\n",
    "    device,\n",
    "    base_optimizer,\n",
    "    task,\n",
    "    loader_dict,\n",
    "    val_table,\n",
    "    loss_fn,\n",
    "    entity_table,\n",
    "    epochs=10,\n",
    ")\n",
    "base_model.load_state_dict(base_state_dict)\n",
    "\n",
    "# Evaluate on val and test set\n",
    "eval_model(base_model, loader_dict, \"val\", task, device, val_table)\n",
    "test_metrics = eval_model(base_model, loader_dict, \"test\", task, device, None)\n",
    "part1_top3_scores.append(test_metrics[\"roc_auc\"])\n",
    "\n",
    "# Finally, graph all of the results\n",
    "create_bar_graph(part1_top3_labels, part1_top3_scores, title='ROC AUC on Top 3 Task vs. Different Training Tasks', x_label=\"Model\", y_label=\"ROC AUC on Top 3 Task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we don't see much of a difference from starting from random weights to using a model pre-initialized from another entity classification task.\n",
    "\n",
    "### Challenge\n",
    "Does this trend necessarily work on larger and more diverse datasets? Depending on your compute availability, try out using different datasets, like `rel-amazon`, as well as across different types of tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Different expressiveness of node features?\n",
    "Next, let's take a look at using different embedding models for node features.\n",
    "\n",
    "The embedding models are used to help turn the tabular data into usable node features. In the Relbench tutorial, the team uses GloVe embeddings, but the paper also mentions utilizing BERT-style embeddings. In traditional NLP, BERT embeddings are much more popular given that they are contextual -- the vector representation depends on the surrounding words, compared to static embeddings used by GloVe -- and can handle words outside of their vocabulary. In addition, their embedding size is $768$ compared to GloVe's $300$, which introduces an opportunity for more expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an investigation, let's switch out our GloVe embedding model with BERT and retrain a new model from scratch on the `driver-dnf` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings.bert import BertTextEmbedding\n",
    "\n",
    "dataset, task, train_table, val_table, test_table = initialize_task(\n",
    "    \"rel-f1\", \"driver-dnf\"\n",
    ")\n",
    "\n",
    "# Preprocess the database data and set up our text embedder\n",
    "db, col_to_stype_dict = db_to_graph(dataset)\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=BertTextEmbedding(device=device), batch_size=128\n",
    ")\n",
    "\n",
    "# Load in data used to train model\n",
    "data, col_stats_dict = make_pkey_fkey_graph(\n",
    "    db,\n",
    "    col_to_stype_dict=col_to_stype_dict,\n",
    "    text_embedder_cfg=text_embedder_cfg,\n",
    "    cache_dir=os.path.join(root_dir, f\"rel-f1_materialized_cache\"),\n",
    ")\n",
    "loader_dict, entity_table = get_loader(train_table, val_table, test_table, task, data)\n",
    "\n",
    "# Initialize new, untrained model using BERT embeddings\n",
    "bert_model = RDLModel(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=2,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(device)\n",
    "bert_optimizer = torch.optim.Adam(bert_model.parameters(), lr=0.005)\n",
    "bert_state_dict = training_run(\n",
    "    bert_model,\n",
    "    device,\n",
    "    bert_optimizer,\n",
    "    task,\n",
    "    loader_dict,\n",
    "    val_table,\n",
    "    loss_fn,\n",
    "    entity_table,\n",
    "    epochs=10,\n",
    ")\n",
    "bert_model.load_state_dict(bert_state_dict)\n",
    "\n",
    "# Evaluate on val and test set\n",
    "eval_model(bert_model, loader_dict, \"val\", task, device, val_table)\n",
    "eval_model(bert_model, loader_dict, \"test\", task, device, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ultimately don't see that drastic of a difference between using BERT embeddings and GloVe embeddings. Despite being trained differently, the fact that the models are close in size and perform similarly on [general embedding benchmarks](https://huggingface.co/spaces/mteb/leaderboard) may suggest that the results will not be that drastic. \n",
    "\n",
    "### Challenge\n",
    "We encourage you to try larger models with even larger embedding dimensions -- to do so, use our `CustomTextEmbedding` class! To use this class, import it as below, and then specify the name of a model as used on HuggingFace:\n",
    "\n",
    "```python\n",
    "from src.embeddings.custom import CustomTextEmbedding\n",
    "text_embedder_cfg = TextEmbedderConfig(\n",
    "    text_embedder=CustomTextEmbedding(model_name=<INSERT_HUGGINGFACE_MODEL_HERE>, device=device), batch_size=128\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Different RDL model architectures?\n",
    "Finally, we experiment with different RDL model architectures. In particular, we investigate what happens as we add or subtract GNN layers from our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Number of GNN Layers\n",
    "\n",
    "First, we double the number of GNN layers in our RDL pipeline, moving from `num_layers=2` to `num_layers=4`. The idea is that by adding more layers, we can create a more expressive network that can understand more complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new model, don't load in old weights.\n",
    "deep_model = RDLModel(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=4,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(device)\n",
    "deep_optimizer = torch.optim.Adam(deep_model.parameters(), lr=0.005)\n",
    "deep_state_dict = training_run(\n",
    "    deep_model,\n",
    "    device,\n",
    "    deep_optimizer,\n",
    "    task,\n",
    "    loader_dict,\n",
    "    val_table,\n",
    "    loss_fn,\n",
    "    entity_table,\n",
    "    epochs=10,\n",
    ")\n",
    "deep_model.load_state_dict(deep_state_dict)\n",
    "\n",
    "# Evaluate on val and test set\n",
    "eval_model(deep_model, loader_dict, \"val\", task, device, val_table)\n",
    "eval_model(deep_model, loader_dict, \"test\", task, device, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, training using $4$ layers actually makes the model perform worse over time. Thus, given the simplicity of the task and the size of dataset, it is likely that we are overfitting. In particular, this is because of over-smoothing. Recall that a receptive field is the set of nodes that determine the embedding of a node of interest. However, if we have too many layers, we might have it so that all node embeddings converge to the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that using double the amount of layers leads to less optimal results, we can try the opposite strategy and halve the number of layers in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new model, don't load in old weights.\n",
    "shallow_model = RDLModel(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=1,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(device)\n",
    "shallow_optimizer = torch.optim.Adam(shallow_model.parameters(), lr=0.005)\n",
    "shallow_state_dict = training_run(\n",
    "    shallow_model,\n",
    "    device,\n",
    "    shallow_optimizer,\n",
    "    task,\n",
    "    loader_dict,\n",
    "    val_table,\n",
    "    loss_fn,\n",
    "    entity_table,\n",
    "    epochs=10,\n",
    ")\n",
    "shallow_model.load_state_dict(shallow_state_dict)\n",
    "\n",
    "# Evaluate on val and test set\n",
    "eval_model(shallow_model, loader_dict, \"val\", task, device, val_table)\n",
    "eval_model(shallow_model, loader_dict, \"test\", task, device, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, we see that even with half the number of layers, we do just about the same as with double the number of layers. Once again, this might be more task-specific as opposed to a general conclusion about GNNs and the RDL pipeline.\n",
    "\n",
    "Finally, we try one last experiment, where we iterate the number of layers from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.viz import plot_roc_auc_vs_layers\n",
    "\n",
    "# Iterate for each number of layers\n",
    "layers_list = []\n",
    "roc_auc_scores = []\n",
    "for layers in range(1, 11):\n",
    "    print(f\"\\nNumber of Layers = {layers}\")\n",
    "\n",
    "    # Define a new model, don't load in old weights.\n",
    "    num_model = RDLModel(\n",
    "        data=data,\n",
    "        col_stats_dict=col_stats_dict,\n",
    "        num_layers=layers,\n",
    "        channels=128,\n",
    "        out_channels=1,\n",
    "        aggr=\"sum\",\n",
    "        norm=\"batch_norm\",\n",
    "    ).to(device)\n",
    "    num_optimizer = torch.optim.Adam(num_model.parameters(), lr=0.005)\n",
    "    num_state_dict = training_run(\n",
    "        num_model,\n",
    "        device,\n",
    "        num_optimizer,\n",
    "        task,\n",
    "        loader_dict,\n",
    "        val_table,\n",
    "        loss_fn,\n",
    "        entity_table,\n",
    "        epochs=10,\n",
    "    )\n",
    "    num_model.load_state_dict(num_state_dict)\n",
    "\n",
    "    # Evaluate on val and test set\n",
    "    val_metrics = eval_model(num_model, loader_dict, \"val\", task, device, val_table)\n",
    "    test_metrics = eval_model(num_model, loader_dict, \"test\", task, device, None)\n",
    "\n",
    "    # Store the number of layers and corresponding test ROC AUC\n",
    "    layers_list.append(layers)\n",
    "    roc_auc_scores.append(test_metrics[\"roc_auc\"])\n",
    "plot_roc_auc_vs_layers(layers_list, roc_auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Does this trend necessarily hold on larger and more diverse datasets? Depending on your compute availability, try out using different datasets, like `rel-amazon`, as well as across different types of tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about different GNN layers?\n",
    "We can also just try using different graph layers, featured in PyG. Below is GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new model, don't load in old weights.\n",
    "from src.models.rdl.gat import RDLGATModel\n",
    "\n",
    "gcn_model = RDLGATModel(\n",
    "    data=data,\n",
    "    col_stats_dict=col_stats_dict,\n",
    "    num_layers=2,\n",
    "    channels=128,\n",
    "    out_channels=1,\n",
    "    aggr=\"sum\",\n",
    "    norm=\"batch_norm\",\n",
    ").to(device)\n",
    "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.005)\n",
    "gcn_state_dict = training_run(\n",
    "    gcn_model,\n",
    "    device,\n",
    "    gcn_optimizer,\n",
    "    task,\n",
    "    loader_dict,\n",
    "    val_table,\n",
    "    loss_fn,\n",
    "    entity_table,\n",
    "    epochs=10,\n",
    ")\n",
    "gcn_model.load_state_dict(gcn_state_dict)\n",
    "\n",
    "# Evaluate on val and test set\n",
    "eval_model(gcn_model, loader_dict, \"val\", task, device, val_table)\n",
    "eval_model(gcn_model, loader_dict, \"test\", task, device, None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPZLbeQ8nLws8bu/8tbPPMT",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
